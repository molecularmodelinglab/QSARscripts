{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b04dea2",
   "metadata": {},
   "source": [
    "# Directions\n",
    "In the cell label Set your files... set the relvant parameters to the values you want them to have. The names should explain what they do, but there are comments to help. Run all the cells in order. The last cell will produce a sdf file that shares the name of your screening sdf file with \"\\_AD\" at the end (test.sdf becomes test_AD.sdf). It will save it the the current working directory unless you specify a save location by setting the save_location variable in the Set cell\n",
    "\n",
    "For reference it took me about 12 minutes to get the 13,000 of 460,000 compound in the AD of a 130 compound training set. This notebook run slower for reasons that I really dont understand but am going to blame on jupyter doing something crazy on the backend so here the runtime roughly doubled to about 20 minutes \n",
    "\n",
    "I spent a unresonable amount of time trying to make sure I protect this script from out of memory errors (screening datasets are too large). So you should be able to run even 10 million+ datasets on this with even 16gb of ram. If it crashes with a memory error, try changing the \"10000\" in np.array_splits function to 100000 (add a zero). If that doesn't work then you need a smaller screeening set and should complain to me that my script has a fatal error and I need to fix it\n",
    "\n",
    "_(note to me, James, or the poor soul that has to maintain this: any out of memery issues is likley to come from the sdf file being too big for the sdf reader in rdkit to read in all at once. Sadly, because rdkit is not very nice there is no way to read it in chunks so you will need to write a wrapper function to split that sdf file into chunks and read them in as indivudal files to save memory)_\n",
    "\n",
    "#### TODOs:\n",
    "I never got the damn multiprocessing to work yet and that could cute runtime down by a factor of 10 if I could get it to work. But there is some shit about PiCkEliNg that python doesn't like and I don't even known why it has to do that to me so like whatever but some day if this puppy runs too slow I'll spend more than 30 minutes and 3 beers trying to figure it out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "493fb97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit.Chem import PandasTools, AllChem\n",
    "from scipy.spatial import KDTree\n",
    "from scipy.spatial.distance import pdist\n",
    "import multiprocessing as mp\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6846b98b",
   "metadata": {},
   "source": [
    "# Set your files and settings here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1a7fedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_file = \"test_zoe.sdf\"  # Fill in your file name here\n",
    "screening_data_file = \"Enamine_Hit_Locator_Library/Enamine_Hit_Locator_Library_HLL-460_460160cmpds_20220221.sdf\"  # Fill in your file name here\n",
    "save_location = os.getcwd()  # set your save location here\n",
    "\n",
    "fingerprint_radius = 5\n",
    "fingerprint_bits = 2048\n",
    "cpu_count = mp.cpu_count() - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9f5865",
   "metadata": {},
   "source": [
    "# Some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6da2579",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_runner(training_tree, screening_rows, cutoff):\n",
    "    screening_rows[\"fp\"] = screening_rows[\"ROMol\"].apply(AllChem.GetMorganFingerprintAsBitVect, \n",
    "                                                         args=(fingerprint_radius, fingerprint_bits))\n",
    "    screening_fp = np.array([list(x) for x in screening_rows[\"fp\"]])\n",
    "\n",
    "    # these are a cool data structure to rapidly find nearest neighbors ask me about them if you want me to fill you in\n",
    "    #  for the non-cs nerds out there:\n",
    "    #     knime fails to utilize something this efficient, so while here we only need to look up the distance of each\n",
    "    #     screening point only once overall, in knime it has to do one look up per training data point. so if I had\n",
    "    #     10,000 datapoint in training and 500,000 in screening with Kdtree I only need 500,000 for knime I need\n",
    "    #     10,000 * 500,000 or 5,000,000,000. Even if you could do 10,000 calculations a second that is still 6 days\n",
    "    #     of compute time compared to 50 seconds for the KDtree version\n",
    "    #  for the cs nerds out there:\n",
    "    #     knime pairwise distance function is O(n**2) while kdtrees are O(n) on build and O(n) on query so O(n) overall\n",
    "    d, i = training_tree.query(screening_fp, workers=-1)\n",
    "\n",
    "    res = np.where(d < cutoff)\n",
    "\n",
    "    return screening_rows.iloc[res]\n",
    "\n",
    "\n",
    "def get_euclidean_threshold(training_fp, mode=1, num_sd=2):\n",
    "    # so there a few ways to do this, take the mean and sd from only nearest neighbors (mode=1) or\n",
    "    #  use all distance pairs (mode=2) both make me feel unconformable as distributions are not defined here\n",
    "    if mode == 1:\n",
    "        training_tree = KDTree(training_fp)\n",
    "        d, i = training_tree.query(training_fp, k=[1, 2], workers=-1)\n",
    "    else:\n",
    "        d = pdist(training_fp)\n",
    "\n",
    "    m = np.mean(d)\n",
    "    sd = np.std(d)\n",
    "\n",
    "    # return the cutoff which is the mean + 2 standard deviations\n",
    "    return m + (num_sd * sd)\n",
    "\n",
    "\n",
    "def get_apd_threshold(training_fp, z=0.5):\n",
    "    # this method is defined in S. Zhang, et. al., J. Chem. Inf. Model., 46 (2006), pp. 1984–1995\n",
    "    #  and in the knime node domain: similarity node from the enalos community\n",
    "    d = pdist(training_fp)\n",
    "    m = np.mean(d)\n",
    "    d_prime = d[np.where(d < m)]\n",
    "    m_prime = np.mean(d_prime)\n",
    "    sd_prime = np.std(d_prime)\n",
    "\n",
    "    # return the cutoff which is the mean + (Z * standard deviation) of lower half set\n",
    "    return m_prime + (z * sd_prime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd561500",
   "metadata": {},
   "source": [
    "# The code to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "206712d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "loaded training: 0.2902233600616455 sec\n",
      "loaded screening 261.14604449272156 sec\n",
      "Done\n",
      "Processing training dataset...\n",
      "Done\n",
      "Beginning AD evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [18:16<00:00,  9.12it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'save_loc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16080\\2074670619.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mnew_file_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscreening_data_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"_AD.sdf\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0msav_loc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_location\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_file_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Saving results to {save_loc}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mPandasTools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWriteSDF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msav_loc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproperties\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'save_loc' is not defined"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "print(\"Loading data...\")\n",
    "t0 = time.time()\n",
    "training_df = PandasTools.LoadSDF(training_data_file)\n",
    "print(\"loaded training:\", str(time.time() - t0), \"sec\")\n",
    "t0 = time.time()\n",
    "screening_df = PandasTools.LoadSDF(screening_data_file)\n",
    "print(\"loaded screening\", str(time.time() - t0), \"sec\")\n",
    "print(\"Done\")\n",
    "\n",
    "print(\"Processing training dataset...\")\n",
    "training_df[\"fp\"] = training_df[\"ROMol\"].apply(AllChem.GetMorganFingerprintAsBitVect, args=(fingerprint_radius, fingerprint_bits))\n",
    "train_fp = np.array([list(x) for x in training_df[\"fp\"]])\n",
    "threshold = get_euclidean_threshold(train_fp)\n",
    "print(\"Done\")\n",
    "\n",
    "print(\"Beginning AD evaluation...\")\n",
    "\n",
    "\n",
    "train_tree = KDTree(train_fp)\n",
    "# NOTE: The 10000 seen in the progress bar is not the number of molecules to use, but the number of chucks to process\n",
    "results = [nn_runner(training_tree=train_tree, screening_rows=x, cutoff=threshold)\n",
    "           for x in tqdm(np.array_split(screening_df, 10000))]\n",
    "print(\"Done\")\n",
    "\n",
    "new_file_name = screening_data_file.split(\"/\")[-1].split(\".\")[0] + \"_AD.sdf\"\n",
    "sav_loc = os.path.join(save_location, new_file_name)\n",
    "print(f\"Saving results to {sav_loc}\")\n",
    "df = pd.concat(results, ignore_index=True, axis=0)\n",
    "PandasTools.WriteSDF(df, sav_loc, properties=list(df.columns))\n",
    "print(\"Done\")\n",
    "print(f\"Overall time: {time.time() - t1} sec\")\n",
    "\n",
    "\"\"\"\n",
    "# pythons ability to multiprocess is bad\n",
    "print(\"Beginning AD evaluation...\")\n",
    "with joblib.parallel_backend('multiprocessing'):\n",
    "    Parallel(n_jobs=cpu_count)(delayed(partial(nn_runner, training_fp=train_fp.copy(), cutoff=threshold))(x)\n",
    "                               for x in tqdm(np.array_split(screening_df, 50)))\n",
    "print(\"Done\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80b71b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
